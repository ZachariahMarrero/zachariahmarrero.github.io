---
title: "Website Overview"
css: about.css
id: about_text
---

::: {#about_text}
## **Keeping it short and simple...**

This website is a place where I will share how to do things with data.

At the top of the page you can currently see a the words 'Accelerated Statistics.' The content within covers some very basic, yet very important, lessons for statistical analysis. Feel free to take a break from the current conversations about AI to review the contents here. You will be surprised at the fun you can have once you understand the tools within.

Furthermore, I will use a similar approach in the presentation and evaluation of popular machine learning algorithms in the weeks, months, and years ahead. I'm currently sitting on a mountain of notes that I've accumulated over the course of my PhD studies (If you want to appreciate the flavor of this work, you can find my dissertation currently through my linkedin in the projects section). I will share methods and materials on how to approach the study of machine learning algorithms. In short, not algorithms are built the same and, currently, there is relatively little work demonstrating the properties of them. In my understanding of it, the math is still being discovered today. Fortunately, classical statistics provides us with a roadmap for how we can approach the evaluation of algorithms in the most general sense AND modern computers make it possible.

The next post I will make, will cover a 'general-purpose' pipeline for running the [XGBoost](https://arxiv.org/pdf/1603.02754.pdf) algorithm (a personal favorite). The post will show how to optimize this algorithm using a Bayesian Optimization routine comprised of a Gaussian Process with a Spectral mixture kernel and the Covariance matrix adaptation evolution strategy.

## A little about XGBoost:

XGBoost is an extremely value tool to have in your analysis toolkit for multiple reasons.

For one, it's extremely expressive. This means it can successfully estimate predictive models in a wide variety of domains. Basically, if you can fit it into a matrix, XGBoost can do something with it.

For two, the X in XGBoost has come to stand for eXtreme. It is blazing fast both in model training and inference. For example, inference on an XGBoost model is O(nklog(d)), where k is the number of trees and d is the depth. So, with 1,000 datapoints, 100,000 trees, and a depth of 30 (which is absurdly large), there are only about 147.7 million operations. In contrast, inference on a simple feedfoward neural network is O(nw), where w is the number of weights. So, again if you have 1,000 datapoints but only 1 million parameters in the network (which is tiny these days) you get 1 billion operations.
:::
