---
title: "XGBoost Scripts"
css: XGBoost_page.css
id: about_text
toc: true
---

::: {#about_text}

### An update about my XGBoost post
A couple weeks ago I indicated I would be making a post which shows how to implement XGBoost with Bayesian Optimization. Here, I have posted two working versions of such a routine. While the scripts are ready, the accompanying post is not. Unlike 'Accelerated Statistics', there's a lot more complexity involved. Describing it in a high-fidelity form while also assuming little on the reader will take some time. So a review post will come in the future. That said, the files do contain lots and lots of notes to help you grasp the broad ideas.

There are 3 files below containing the scripts to execute the routines in both Python and R. They are not identical, as you can read below, but in both scripts, Bayesian Optimization is used for hyperparameter tuning. Also, both scripts have different kinds of complimentary information covering different aspects of the routines. Whereas the Python script will give you a better sense of how Bayesian optimization routines work, the R script will provide more insight into what the XGBoost hyperparameters do. As a personal preference, I would default to using the Python script because it is more advanced/powerful/expressive.

[**Read the XGBoost paper**](https://arxiv.org/pdf/1603.02754.pdf) 

## Why XGBoost:

XGBoost is an extremely value tool to have in your analysis toolkit for multiple reasons.

For one, it's extremely expressive. This means it can successfully estimate predictive models which cover a wide variety of forms. Whereas for a linear model we typically estimate model parameters of a single function that we have specified,in XGBoost, we don't define the function. Instead, each tree is a function drawn from a space of functions. This 'piecemeal' design is akin to 'growing' your the function. As a result, the algorithm is only limited by the types of functions that can fit within a tree structure (which is such an enormous variety that it's hard to define a class of function for which the algorithm will simply fail). Put differently, if you can fit it into a matrix, XGBoost is often the first best step.

For two, the X in XGBoost stands for eXtreme. It is blazing fast both in model training and inference. For example, inference on an XGBoost model is O(nklog(d)), where k is the number of trees and d is the depth. So, with 1,000 datapoints, 100,000 trees, and a depth of 30 (which is absurdly large), there are only about 147.7 million operations. It can also process this on GPUs. For comparison, inference on a simple feedfoward neural network is O(nw), where w is the number of weights. So, again if you have 1,000 datapoints but only 1 million parameters in the network (which is tiny these days) you get 1 billion operations. 

## **XGBoost in Python**

This version of the routine will use a customized version of cross-validation which is processed asynchronously. Doing so maximizes compute resource utilization. It also uses a superior Gaussian Process modeling approach as the surrogate model for the Bayesian Optimization, along with the Spectral Mixture Kernel.  For candidate points, it uses One-Shot Knowledge Gradient rather than Covariance Matrix Adaptation Evolutionary Search (CMAES). Within the file there is an explanation as to the differences between the two. It's also the case that BoTorch can use and accelerate CMAES via GPU too.

1. [XGBoost in Python Script](../code/XGB_OKG_SpectralMixtureKernel/XGBPython.py){download="XGBPython.py"}
2. [Worker.py script (needed for XGBoost in Python)](../code/XGB_OKG_SpectralMixtureKernel/worker.py){download="worker.py"}

To use the python script, download both files above.  Place them in the same directory for simplicity.  


> *Sidenote:* 
>
>This script uses GPU so you'll want to ensure that you have the CUDA and cuDNN drivers installed. There are tutorials for this online. Mostly, you just follow these steps: download and install the CUDA driver.  Then download the cuDNN driver, unpack the zip file, when you open the folder there will be subfolders. For each of those subfolders, you need to copy/paste the files within to the folder which has the same name in the CUDA folders. That's all. Then, you'll need PyTorch with GPU capability.  If you go to the PyTorch website they have a clickable interface which will construct the appropriate download call for doing so.  You copy the call to a command prompt and that's all you need to do.   


## **XGBoost in R**
3. [XGBoost in R](../code/XGBoostinR.R)

This version of XGBoost is strictly CPU only.  The models will process in parallel on each core of the CPU but no GPU activity will be involved. While it is not as advanced as the Python implementation, it is still extremely expressive.  It has several drawbacks. For instance, the Spectral Mixture Kernel is not used (because it's not defined in the associated package and writing this out would be a headache with relatively little upside compared to just using the freely available Python version). Additionally, there is no GPU to speed up estimation. So, running more than a few hundred steps in the routine will incur a huge processing cost. Traditional Gaussian Processes require O(n^3) operations. I once ran this for 2,500 steps. It took 11 days on a Ryzen 9 5950x!!! Essentially 99% of the compute time was the Gaussian Process models getting estimated. Sooooo, in other words, it scales terribly. BUT! you don't actually have to run that many steps to get acceptable results. 

:::
