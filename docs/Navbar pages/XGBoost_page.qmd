---
title: "XGBoost Scripts"
css: XGBoost_page.css
id: about_text
toc: true
---

::: {#about_text}

### An update about my XGBoost post
A couple weeks ago I indicated I would be making a post which shows how to implement XGBoost with Bayesian Optimization. At this point I have two working versions of this routine. They are not as extremely well documented as my post in 'Accelerated Statistics'. There's just too much going on to do a thorough enough job for that at this time. So a review post will come in the future. That said, the files do contain lots and lots of notes to help you along. 

There are 3 files below which contain scripts that will enable you to fit XGBoost models in both Python and R. Each is different as you can read below but in both scripts, Bayesian Optimization is used for hyperparameter tuning. Also, both scripts have different kinds of complimentary information that can help you understand  the Python script will give you a better sense of how the Bayesian optimization routines work. As a personal preference, I would default to using the Python script because it is more advanced/powerful/expressive.

[**Read the XGBoost paper**](https://arxiv.org/pdf/1603.02754.pdf) 

## Why XGBoost:

XGBoost is an extremely value tool to have in your analysis toolkit for multiple reasons.

For one, it's extremely expressive. This means it can successfully estimate predictive models which cover a wide variety of forms. Basically, whereas in a linear model we are estimating model parameters, in XGBoost, each tree is a function drawn from a space of functions. This 'piecemeal' design is akin to 'growing' your final function. Put differently, if you can fit it into a matrix, XGBoost is often the first best step.

For two, the X in XGBoost has stands for eXtreme. It is blazing fast both in model training and inference. For example, inference on an XGBoost model is O(nklog(d)), where k is the number of trees and d is the depth. So, with 1,000 datapoints, 100,000 trees, and a depth of 30 (which is absurdly large), there are only about 147.7 million operations. It can also process this on GPUs. For comparison, inference on a simple feedfoward neural network is O(nw), where w is the number of weights. So, again if you have 1,000 datapoints but only 1 million parameters in the network (which is tiny these days) you get 1 billion operations. 

## **XGBoost in Python**

This version of the routine will use a customized version of cross-validation.  It also uses a superior Gaussian Process modeling approach along with the Spectral Mixture Kernel.  For candidate points, it uses One-Shot Knowledge Gradient rather than Covariance Matrix Adaptation Evolutionary Search. Within the file there is an explanation as to the differences between the two. 

1. [XGBoost in Python Script](../code/XGB_OKG_SpectralMixtureKernel/XGBPython.py){download="XGBPython.py"}
2. [Worker.py script (needed for XGBoost in Python)](../code/XGB_OKG_SpectralMixtureKernel/worker.py){download="worker.py"}

To use the python script, download both files above.  Place them in the same directory for simplicity.  


> *Sidenote:* 
>
>This script uses GPU so you'll want to ensure that you have the CUDA and cuDNN drivers installed. There are tutorials for this online but it's mostly just download and install the CUDA driver.  Then download the cuDNN driver, unpack the zip file, copy/paste the files within the folders of the unpacked zip file to the folder which has the same name in the CUDA folders.  


## **XGBoost in R**
3. [XGBoost in R](../code/XGBoostinR.R)

This version of XGBoost is strictly CPU only.  The models will process in parallel on each core of the CPU but no GPU activity will be involved. While it is not as advanced as the Python implementation, it is still extremely expressive.  It has several drawbacks. For instance, the Spectral Mixture Kernel is not used (because it's not defined in the associated package and writing this out would be a headache with relatively little upside compared to just using the freely available Python version). Additionally, there is no the GPU to speed up estimation and running more than a few hundred steps in the routine will incur a huge processing cost. Traditionally Gaussian Processes require O(n^3) operations. I once ran this for 2,500 steps. It took 11 days on a Ryzen 9 5950x!!! Essentially 99% of the compute time was the Gaussian Process models getting updated. In other words, it scales terribly. BUT! you don't actually have to run that many steps to get acceptable results. 

:::
